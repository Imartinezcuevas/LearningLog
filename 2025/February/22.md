Date: Saturday, February 22, 2025

# What I learned today:

## Reinforcement learning

The reward is a signal received by the agent and will always be numerical values. Rewards are fundamental for evaluating action sequences. The higher the reward, the better the sequence of actions was. From here arises the reward hypothesis.

### Reward Hypothesis
Any objective can be formalized as the maximization of the expected value of the accumulated expected sum of rewards.
**That is, by merely summing rewards obtained from a sequence of actions, we achieve agent learning.**
The idea is to be able to optimize this sequence.

### Actions
Actions are the decisions or movements that the agent can take at any moment during the task.

**A policy is a way for the agent to make decisions.** The policy is a mapping from states to actions. We have two cases:
* Deterministic policy: the action to be taken given a state is always clear. This is the ideal objective. Not being conditioned.
* Stochastic policy: it will be a probability over actions for a specific state. From that state, there will be a probability distribution, which ideally we already know, but in practice, this doesn't occur. In such a way that we will take the action that can guarantee us the highest possible return.

### History
History is the sequence of observations, actions, and rewards. These are all observable variables up to time t.

### States
The difficulty lies in how we define the states. Since states don't always have to be a complete observation of the entire environment configuration.

Decision processes with hidden variables are not addressed.

We will always have the ability to access all data. Completely: $O_t = O^a_t = S^e_t$

### Reinforcement Learning
Within the context of reinforcement learning, there are two types of problems:
1. Reinforcement learning as such, which is obtaining experience to estimate certain value functions.
2. Building models from experience -> planning.

If we want to obtain a good general reinforcement learning technique, we must use both.

### Difference between exploitation and exploration
* Exploitation: uses known information to maximize reward.
* Exploration: finding more information about the environment.
We must seek a balance between actions we already know are good and trying new options.

### Types of agents
* Value-based: these are algorithms based on using state value functions. And therefore, if we have the state value function, we are able to determine which policy we should implement. **Maximize value - Greedy.**
* Policy-based: a policy is learned without taking into account state values. Agents are obtained that store the policy and try to optimize it. **Gradient ascent.**
* Actor-critic: someone optimizes the policy and sees how good that policy is.

### Evaluative vs. instructive feedback
Instead of telling us how good an action is, we are given an evaluation of the actions taken. The system doesn't tell us what we should do, it only tells us if we did well or poorly.
* Evaluative feedback: how good an action taken was, but not whether it was the best or not.
* Instructive feedback: the system tells us what we should have done.

## Multi-armed bandit problem
We have k-possible options and from these k-possible options, I will obtain an evaluation. From that evaluation, I have to resume the situation and take a new action again.

The objective is to maximize the total reward in a time period that we select.

The actions I take do not alter the situation of the environment.

### Round-Robin
Method that distributes resources or tasks equitably or cyclically among various participants, processes, or teams.

### Action-value - Sample average method
It involves establishing values for actions. They estimate reward distributions.

How I estimate an expected value: through an average.
$$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \cdot I_{A_i=a}}{\sum_{i=1}^{t-1} I_{A_i=a}}$$

### Greedy algorithm
Choose the action that has the highest associated reward.
$$A_t = arg_a \text{  max  } Q_t(a)$$
Problem: it only performs exploitations and makes no attempt to explore.

### Epsilon-greedy algorithm
They are almost always greedy except sometimes with probability $\epsilon$.

#### Optimistic initial values method
Estimates are initialized to a very high value (extremely optimistic). This will ensure that we select all arms at least once, since they all have a very high estimate before pulling, and when we pull and adjust the estimate, it may decrease and we select another arm.

With this, it is expected that there will be great exploration at the beginning and then focus on exploitation.

### Epsilon-decay algorithm
$\epsilon$ changes during execution. Initially it is a high value and decays during execution. As the agent accumulates experience, the goal is to make greater exploitation of the best known actions and reduce the probability of exploration.

### How to measure the efficiency of an algorithm?
If we have 10 possible actions and one is the best of all, when will the algorithm be good? Whenever it chooses the best.

The idea to measure how good an algorithm is by itself is to calculate: the expected value in the most optimal arm and subtract from it the reward obtained when getting an arm at instant i. If that difference is 0, it means that the optimal arm has been selected. This is regret.
$$R_t = q*T - \sum_{t=1}^{T} r_t$$

Normally we don't know this. Therefore, we work in terms of expected values.

### UCB Methods
There are probability distributions such that if they meet certain conditions, the rejection will be maintained between the lower bound and an upper bound. Furthermore, the upper bound converges to the lower bound.

What is proposed to obtain that upper bound is to consider what we call upper confidence bounds, which is given by the expected value plus a term u(a).
$$ucb(a) = Q(a) + u(a)$$
* Exploitation term, Q(a)
* Exploration term, u(a)

The idea is to select the greediest action to maximize the upper confidence bound. $a_t = arg \text{  max  } ucb(a)$

When will we select an action that meets this condition? For two reasons:
1. The u(a) is small and then, Q(a) is large. We focus on arms with high rewards. **Exploitation**
2. u(a) is large and then, Q(a) is small. We are optimistic. **We explore**

#### UCB1
We use Hoeffding's inequality.

UCB1 establishes $\frac{2}{\alpha} = t^{-4}$, with t being the total number of actions. With which we obtain that $u(a) = \epsilon = \sqrt{\frac{2 lnt}{N_t(a)}}$.
With this we would have the following:
$$ucb(a) =  Q_t(a) + \sqrt{\frac{2 lnt}{N_t(a)}}$$

Actions that are not chosen, as time increases their upper bound increases. That is, we give them more confidence to be chosen.
If chosen, u(a) would decrease.

#### UCB2
$$ucb(a) = Q(a) + \sqrt{\frac{(1+\alpha) ln(\frac{t}{\tau(k_a)})}{2\tau(k_a)}}$$
* $k_a$: number of epochs of action a
* $\tau(k_a)$: determines the number of times action a will be selected in an epoch.

What is an epoch?
An epoch is a period of time in which an action is selected.

We go through each instant of time, for each instant we select that action given by the upper expression and determine the epoch that corresponds to it. That is, the number of times it will be executed.

### Gradient ascent methods
#### Softmax method
In the exploration base of $\epsilon$-greedy, bandits are selected randomly. The change is to assign a probability to each action proportional to the value of the current expected reward. That's why we use the softmax distribution, we transform numbers into probability distributions.

#### Preference gradient method
Instead of working with the expected value, one can work with a preference function over actions. Select according to a sampling to a probability distribution, but now it depends on the function H(a).

$H(a)$ is a numerical preference for each action a. The preference of that action is increased based on the number of times it has been selected and the others decrease.