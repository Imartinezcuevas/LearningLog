Date: Wednesday, February 19, 2025

# What I learned today:

## Reinforcement learning
### What is the k-arm bandit problem?
It is a classic reinforcement learning problem where an agent must make decisions in an uncertain environment to maximize its reward over time.

#### Analogy with a slot machine
Imagine we are in a casino and there are k different slot machines. Each machine has an unknown reward distribution:
* Some pay more often.
* Other pay less frequently, but with larger rewards.

The objective is find the machine that gives us the greatest profit in the long term, but without knowing wich one it is at the beggining. To achieve this, we must decide between:
1. Exploration ->  Try different machines to find out which one is the best.
2. Exploitation -> Play the machine that you think gives the best reward based on what you already know.

This dilemma between exploration and exploitation is the heart of the problem.

### Mathematically, how is it modeled?
The problem is defined with the following elements:
 * $k$ ->  Number of arms available.
 * $A_t$ -> Action chosen over time t (which arm we pull).
 * $R_t$ -> Reward obtained over time t after choosing $A_t$ 
 * $Q*(a)$ -> Expected real value of the arm a (unknown to the player).

 
Each arm has an unknown reward distribution $P(R_t|A_t = a)$, and the objective is to maximize the total reward.

### How do we solve this problem?
There are several algorithms that try to balance exploration and exploitation:
1. Epsilon-Greedy
* With probability Ïµ â†’ Explore a random arm.
* With probability 1âˆ’Ïµ â†’ Exploitation: choose the arm with the best average observed reward.

    ðŸ’¡It's simple and works well, but it's not always optimal.
2. Upper Confidence Bound (UCB)
* Consider uncertainty in addition to average reward.
* Prefiere explorar brazos con menos informaciÃ³n.
* Basado en teorÃ­a de optimizaciÃ³n matemÃ¡tica.

    ðŸ’¡It is more efficient than epsilon-greedy in stationary environments.

3. Gradient ascent (softmax y preference gradient)
 * It doesn't use average rewards, but instead learns arm preferences.
 * It uses a method called gradient ascent to adjust the probability of choosing each arm.

    ðŸ’¡It is useful when rewards change over time (non-stationary environments).