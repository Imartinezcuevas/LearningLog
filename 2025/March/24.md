Date: Monday, March 24, 2025

# What I learned today:

## Explainable Machine Learning

**What are SHAP values and how do they help interpret a model?**

SHAP (Shapley Additive Explanations) scores are a technique for explaining a model's predictions locally. They assign each feature a value that reflects its impact on the final prediction, based on game theory, specifically Shapley scores. This allows us to understand how each variable contributes to the prediction of a specific instance, providing a detailed interpretation of each prediction.+

SHAP scores are a way of assigning a measure of importance to each feature (or variable) in a model to understand how each contributes to a specific prediction. That is, they tell you how each individual feature affects the output of a prediction compared to the model's average or baseline prediction.

**Are SHAP values ​​a global or local technique?**

SHAP scores can be used at both the local and global levels. At the local level, SHAP scores explain the individual predictions of instances in the dataset, allowing you to understand how each feature contributes to a specific prediction. At the global level, the SHAP scores of all instances can be aggregated to gain an overview of the importance of each feature across the dataset.

**How are SHAP values ​​calculated?**

SHAP values ​​are calculated differently depending on the model type:

* For models with a clear structure (such as decision trees), optimized algorithms such as TreeSHAP are available that allow SHAP values ​​to be calculated accurately.
* For black-box models (such as neural networks or complex models), SHAP values ​​are estimated using Monte Carlo-based approximations, evaluating the impact of each variable on the prediction.

**How do you interpret SHAP values ​​for a specific instance? What does a positive or negative SHAP value mean?**

For a specific instance, SHAP values ​​indicate how much each feature contributes to the model's prediction compared to the average prediction.

* A positive SHAP value means that the feature increases the probability of the model predicting the target class.
* A negative SHAP value means that the feature reduces the probability of the model predicting the target class.