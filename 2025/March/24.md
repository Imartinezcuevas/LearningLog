Date: Monday, March 24, 2025

# What I learned today:

## Explainable Machine Learning

**What are SHAP values and how do they help interpret a model?**

SHAP (Shapley Additive Explanations) scores are a technique for explaining a model's predictions locally. They assign each feature a value that reflects its impact on the final prediction, based on game theory, specifically Shapley scores. This allows us to understand how each variable contributes to the prediction of a specific instance, providing a detailed interpretation of each prediction.+

SHAP scores are a way of assigning a measure of importance to each feature (or variable) in a model to understand how each contributes to a specific prediction. That is, they tell you how each individual feature affects the output of a prediction compared to the model's average or baseline prediction.

**Are SHAP values ​​a global or local technique?**

SHAP scores can be used at both the local and global levels. At the local level, SHAP scores explain the individual predictions of instances in the dataset, allowing you to understand how each feature contributes to a specific prediction. At the global level, the SHAP scores of all instances can be aggregated to gain an overview of the importance of each feature across the dataset.

**How are SHAP values ​​calculated?**

SHAP values ​​are calculated differently depending on the model type:

* For models with a clear structure (such as decision trees), optimized algorithms such as TreeSHAP are available that allow SHAP values ​​to be calculated accurately.
* For black-box models (such as neural networks or complex models), SHAP values ​​are estimated using Monte Carlo-based approximations, evaluating the impact of each variable on the prediction.

**How do you interpret SHAP values ​​for a specific instance? What does a positive or negative SHAP value mean?**

For a specific instance, SHAP values ​​indicate how much each feature contributes to the model's prediction compared to the average prediction.

* A positive SHAP value means that the feature increases the probability of the model predicting the target class.
* A negative SHAP value means that the feature reduces the probability of the model predicting the target class.

**Why does relying on game theory guarantee more consistent and fair explanations?**

Relying on game theory guarantees more consistent and fair explanations because Shapley scores comply with key mathematical properties that ensure an equitable distribution of the importance of each feature in the model's prediction.

The three fundamental properties that make SHAP more reliable are:
1. Efficiency: The sum of all contributions (SHAP scores) is equal to the difference between the model's prediction and the mean of the predictions, ensuring no loss or excess contribution.
2. Monotonicity: If a feature has a greater impact across all possible combinations, its SHAP score will be higher. This ensures that the importance of variables is assigned logically.
3. Absence of Contribution: If a feature does not change the model's prediction in any case, its SHAP score will be zero, ensuring that only relevant features influence the explanation.

In contrast, techniques such as LIME do not meet these properties, as they approximate explanations locally with linear models, which can lead to inconsistencies in the values ​​assigned to features.

## Federated Learning
### Comparison between DFL and CFL with IID and Non-IID Data

In the experiments conducted, there are noticeable differences in the behavior of models trained with IID and Non-IID data. These differences can be attributed to several factors, including how data is distributed, how models converge, and the architecture of the communication process. Below are the key points in more detail:

#### 1. Accuracy and Convergence

- **IID Data**: When training models with IID (Independent and Identically Distributed) data, the behavior is generally more stable and predictable. The models achieve slightly higher accuracy levels compared to Non-IID data. This can be attributed to the fact that, in IID scenarios, the data distribution is more uniform across clients. This uniformity allows the models to converge faster and more consistently across different clients. In addition, since the data on each client is representative of the global dataset, it is easier for the models to generalize and avoid overfitting to local patterns. Therefore, the models tend to converge to similar values, leading to better overall performance.

- **Non-IID Data**: On the other hand, when working with Non-IID data, the models show greater variability in their behavior. This results in higher divergence in the convergence of different clients. Non-IID data presents challenges because each client has data that is drawn from a different distribution, meaning that the data is more heterogeneous. As a result, the models tend to specialize in their local data, which can make it harder for them to generalize well during the aggregation phase. The varying distributions of data across clients lead to inconsistent updates, which in turn causes greater divergence in the final model's parameters. This can reduce the overall effectiveness of the federated learning process.

   **Possible causes of these differences**:
   - *Data Distribution*: In Non-IID scenarios, each client has a different data distribution, which complicates the model’s ability to generalize across the whole dataset. Unlike IID data, which is homogenous and distributed similarly across clients, Non-IID data creates a challenge for aggregation, as the models must adjust to the unique characteristics of each client’s data.
   - *Local Biases*: Non-IID data encourages models to specialize in local data, which creates local biases. These biases can be problematic during model aggregation, as the federated learning process requires a more generalized model that can effectively perform on data from all clients. These biases may lead to issues where the global model becomes overfitted to the local data, thus degrading the performance when deployed on a broader dataset.

#### 2. Effect of Centralization

- When comparing DFL (Decentralized Federated Learning) and CFL (Centralized Federated Learning) models, there is no significant difference observed in terms of the impact of centralization on the accuracy of the models. The centralization of the communication process, where a central server is responsible for aggregating model updates, does not drastically change the results between the two types of federated learning, regardless of whether the data is IID or Non-IID. This indicates that the centralization aspect may not be a dominant factor in determining the model's accuracy in this particular experiment.

- However, it's important to note that while centralization may not drastically affect accuracy, it does play a crucial role in the communication structure and efficiency of the federated learning system. Centralized systems may introduce certain inefficiencies, especially in terms of communication load and model synchronization, which can be influenced by the underlying data distribution.

#### 3. Bytes Received

- **DFL (Decentralized Federated Learning)**: In DFL, the bytes received by the clients are more evenly distributed, whether the data is IID or Non-IID. Since DFL involves a decentralized communication structure, where nodes communicate with one another instead of relying on a central server, the load is shared more equally across all clients. This results in a more homogeneous distribution of the communication workload, which can be beneficial in terms of load balancing and reducing bottlenecks in the system.

- **CFL (Centralized Federated Learning)**: In CFL, there is a noticeable disparity in the bytes received by different clients. Some clients end up receiving significantly more bytes than others. This occurs because, in CFL, the central server acts as the focal point for all communication. Every client sends updates to the server, and the server aggregates these updates to send back to the clients. Due to this centralized structure, clients that are located further from the server or have more data to update may require more communication, resulting in an unequal distribution of communication load. 

   **Possible causes of these differences**:
   - *Network Architecture*: The fundamental difference between DFL and CFL lies in the communication architecture. In DFL, since the communication is decentralized, clients communicate directly with each other, leading to a more balanced distribution of the communication load. In contrast, CFL relies on a central server that handles all the communication, which can cause a bottleneck at the server and a significant disparity in the amount of data exchanged between clients and the server.
   - *Data Heterogeneity*: In scenarios with Non-IID data, some clients may have more challenging or divergent data compared to others. This may require more communication to synchronize models across clients. In CFL, when models differ significantly due to the Non-IID data distribution, clients with more complex models or those with highly specialized data may need to exchange more updates with the central server, leading to an increase in the number of bytes received by those clients.

   **Additional Considerations**:
   - In scenarios with highly Non-IID data, the central server may struggle to effectively aggregate models that have diverged significantly. As a result, the central server may require more frequent updates from certain clients to maintain synchronization, leading to higher communication costs for those specific clients.
   - In contrast, in DFL, since nodes exchange information with each other directly, the communication burden is spread across the network more evenly. This decentralized structure helps mitigate the problem of central bottlenecks and leads to more efficient use of network resources.

