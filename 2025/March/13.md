Date: Thursday, March 13, 2025

# What I learned today:

## DeepStream
DeepStream is based on configurations and plugins, so there are three key components in the structure of an application.

### Configuration files (`.txt` and `.cfg`)
DeepStream uses configuration files to define how the pipeline runs without the need to modify the C++ code.

A typical file looks like this:
```ini
[application]
gst-debug=3 # Nivel de debug de GStreamer

[tiled-display]
enable=1
rows=1
columns=1
width=1920
height=1080

[source0]
uri=file://sample.mp4  # Ruta del video
enable=1
gpu-id=0

[streammux]
batch-size=1
width=1920
height=1080
gpu-id=0
nvbuf-memory-type=0

[primary-gie]
enable=1
model-engine-file=model_b1_fp16.engine  # Modelo convertido con TensorRT
batch-size=1
gie-unique-id=1
config-file=config_infer_primary_peoplenet.txt

[tracker]
enable=1
tracker-width=640
tracker-height=384
gpu-id=0
ll-lib-file=/opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so
```

**Important Sections**
* `[application]`: General pipeline configuration (debug, logging, etc.)
* `[sourceX]`: Video source definition
* `[streammux]`: Multiplexer parameters (for handling multiple streams in parallel)
* `[primary-gie]`: Inference model configuration (nvinfer)
* `[tracker]`: Optional, if you need object tracking

### Inference configuration file
This file configures the AI ​​model in TensorRT.

```ini
[property]
gpu-id=0
net-scale-factor=0.00392156979
model-engine-file=model_b1_fp16.engine  # Modelo compilado con TensorRT
onnx-file=model.onnx                    # Modelo original en ONNX
labelfile-path=labels.txt
batch-size=1
network-mode=2  # FP16
num-detected-classes=1
interval=0
gie-unique-id=1

[class-attrs-all]
pre-cluster-threshold=0.2
```

**Explanation**
* `onnx-file`: Original model path
* `model-engine-file`: Model optimized with TensorRT
* `network-mode`: Model accuracy (FP32=0, FP16=1, INT8=2)
* `batch-size`: Model batch size
* `pre-cluster-threshold`: Confidence threshold before post-processing.

### Custom nvdsinfer Plugin for Post-Processing
If the model has custom output, you need to write your own C++ code for post-processing.

DeepStream uses ``nvinfer`` to perform inference with AI models. However, ``nvinfer`` by default only handles object detection models (bounding boxes).

If we want to use another type of model, such as a depth estimator, we need to:
* Modify how the model output is interpreted.
* Implement post-processing in C++ to convert the results into something useful for DeepStream.

To do this, we will create a custom nvdsinfer plugin in C++.

#### nvdsinfer Structures
The nvdsinfer-related structures are essential for post-processing within a custom plugin. DeepStream interacts with these structures to pass the model's output so your plugin can process it.

**Basic structure used:**
1. `NvDsInferLayerInfo`
* Represents information about an inference model's output layer.
* The model's output is stored in `layer.buffer`.

Important fields:
* `buffer`: A pointer to the model's output data.
* `dims`: Contains the model's output dimensions (e.g., 1x384x384 for a depth map).
2. `NvDsInferNetworkInfo`
* Contains details about the network configuration.
* Used to obtain image size and other model-related parameters.

Important fields:
* `width`, `height`: Dimensions of the input image.

3. `NvDsInferParserDetectionParams`
* Contains parameters for post-processing detections.
4. `NvDsInferObjectDetectionInfo`
* Stores information about object detections.
* DeepStream uses this structure to store detected objects and their characteristics.

### Expected Output and How DeepStream Uses It
The expected output depends on what you do with your custom plugin.
* In the case of a depth estimation model, we expect the plugin to receive a 1x384x384 tensor (the output of MiDaS).
* The plugin then processes the data, normalizes it, and converts it into a visualizable depth image.

If the plugin generates a depth image, DeepStream can use it as a new layer in the video (for example, by adding a depth image overlay over the live video).

### Detailed explanation of the plugin
Example:
```cpp
// El fichero se llamará custom_nvdsinfer_midas.cpp

#include "nvdsinfer_custom_impl.h"
#include <opencv2/opencv.hpp>
#include <iostream>

// Esta es la función principal que DeepStream llamará para procesar los resultados de inferencia
extern "C" bool NbDsInferParseCustomMidas(
    std::vector<NvDsInferLayerInfo> const &outputLayersInfo, //Capa de salida del modelo
    NvDsInferNetworkInfo const &networkInfo,                 // Información de la red
    NvDsInferParseDetectionParams const &detectionParams,    // Parámetros de detección
    std::vector<NvDsInferObjectDetectionInfo> %objectList)   // Lista de objetos detectados
{
    // La salida de nuestro modelo de profundidad es un tensor de 1x384x384
    const NvDsInferLayerInfo &depthLayer = outputLayersInfo[0]; // Seleccionamos la capa de salida del modelo
    const float *depthData = (float *)depthLayer.buffer;        // La salida se encuentra en "buffer"

    int width = 384;  // Ancho del mapa de profundidad (dimensiones del modelo)
    int height = 384; // Altura del mapa de profundidad

    // Convertir la salida en una imagen de OpenCV
    cv::Mat depthMap(height, with, CV_32F, (void*)depthData); //Creamos una imagen de OpenCV a partir de los datos

    // Normalizar la imagen para visualización
    double minVal, maxVal;
    cv::minMaxLoc(depthMap, &minVal, &maxVal); // Encontramos los valores mínimo y máximo de la imagen
    depthMap = (depthMap - minVal) / (maxVal - minVal) * 255.0: // Normalizamos la imagen
    depthMap.convertTo(depthMap, CV_8U); // Convertimos la imagen a tipo CV_8U (escala de grises)

    // Dibujamos la imagen
    cv::Mat depthOverlay(height, width, CV_8UC3); // Creamos una imagen de 3 canales para la superposición
    cv::applyColorMap(depthMap, depthOverlay, cv::COLORMAP_JET); // Aplicamos un mapa de colores para visualización

    // Mostrar en pantalla
    cv::imshow("DepthMap", depthOverlay); //Mostramos la imagen
    cv::waitKey(1); //Esperamos un pequeño intervalo

    return true; //Devolemos true para indicar que todo se procesó correctamente.
}
```

## Real-Time Visualization
DeepStream doesn't have direct support for easily displaying generated images as an overlay, but we can do so through OpenCV.

When a new frame is received, instead of just displaying the original frame, you add the depth map overlay before displaying it.
